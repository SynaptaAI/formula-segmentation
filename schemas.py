from typing import List, Dict, Any, Optional, Union, Literal

from pydantic import BaseModel, Field


class BBox(BaseModel):
    page: int
    x0: float
    y0: float
    x1: float
    y1: float


class ConceptLink(BaseModel):
    concept_id: str
    link_method: Literal["exact_match", "alias", "semantic", "heuristic", "fallback", "llm_rerank"]
    confidence: float
    # Optional metadata copied from the concept catalog to make downstream
    # inspection/debugging easier without a separate join.
    concept_name: Optional[str] = None
    level: Optional[int] = None
    tags: List[str] = Field(default_factory=list)
    rationale: Optional[str] = None


class Edge(BaseModel):
    """Explicit typed edge for KG traversal. source_id -> target_id with edge_type."""
    edge_id: str
    source_id: str
    target_id: str
    edge_type: Literal[
        "DEFINES",
        "EXPLAINS",
        "USES_FORMULA",
        "WORKED_EXAMPLE_OF",
        "REFERENCES",
        "NEAR",
        "ANSWER_OF",
        "DUPLICATE_OF",
    ]
    strength: float = 1.0
    link_method: Optional[Literal["exact", "alias", "semantic", "heuristic"]] = None
    anchor_metadata: Optional[Dict[str, Any]] = None  # page, heading_path, bbox, method, snippet


class VariableDefinition(BaseModel):
    symbol: str
    meaning: str
    units: Optional[str] = None
    inferred: bool = False
    source: Literal["context", "formula_only", "llm", "heuristic"] = "heuristic"


class SegmentBase(BaseModel):
    segment_id: str
    book_id: str
    chapter_number: str
    chapter_title: Optional[str] = None
    page_start: int
    page_end: int
    bbox: Optional[BBox] = None
    text_content: str
    context_before: Optional[str] = None
    context_after: Optional[str] = None
    concept_links: List[ConceptLink] = Field(default_factory=list)
    heading_path: Optional[str] = None  # e.g., "9.1.2 Risk and Return"
    prev_segment_id: Optional[str] = None
    next_segment_id: Optional[str] = None
    doc_uri: Optional[str] = None  # optional: PDF path or URI for multi-doc traceability
    needs_human_review: bool = False


class FormulaSegment(SegmentBase):
    segment_type: Literal["formula"] = "formula"
    formula_text_raw: str = ""  # Dedicated raw text
    formula_latex: Optional[str] = None
    equation_number: Optional[str] = None
    variables: List[VariableDefinition] = Field(default_factory=list)
    usage_type: Literal["definition", "application", "reference"] = "application"
    referenced_formula_ids: List[str] = Field(default_factory=list)
    referenced_example_ids: List[str] = Field(default_factory=list)  # Bidirectional link: examples that use this formula
    canonical_formula_key: Optional[str] = None  # Hash/normalized signature for duplicate detection
    is_duplicate_of: Optional[str] = None
    short_meaning: Optional[str] = None  # "What it calculates" summary
    confidence: float = 1.0


class WorkedExampleSegment(SegmentBase):
    segment_type: Literal["worked_example"] = "worked_example"
    title: Optional[str] = None
    example_prompt: Optional[str] = None  # Problem statement
    given_data: Dict[str, Any] = Field(default_factory=dict)  # {"r": 0.05, "T": 10, ...}
    steps: List[str] = Field(default_factory=list)
    final_answer: Optional[str] = None
    output_variables: List[str] = Field(default_factory=list)  # ["PV", "FV"] - what the example solves for
    referenced_formula_ids: List[str] = Field(default_factory=list)


class QuestionSegment(SegmentBase):
    segment_type: Literal["question"] = "question"
    question_number: str
    subparts: List[Dict[str, str]] = Field(default_factory=list)  # [{"label": "a", "text": "..."}]
    question_type: Optional[Literal["MCQ", "Short Answer", "Calculation", "Case", "Essay", "Mixed"]] = None
    choices: List[str] = Field(default_factory=list)  # For MCQ: ["A. Option 1", "B. Option 2"]
    referenced_formula_ids: List[str] = Field(default_factory=list)
    solution_status: Literal["linked", "not_found_in_book", "ambiguous", "unattempted", "synthetic"] = "unattempted"
    problem_ref_key: Optional[str] = None  # Normalized reference key: "Ch1-Q5" or "1.2-5"
    problem_set_type: Optional[Literal["problem_set", "cfa", "concept_check", "other"]] = None  # Which problem set section


class SolutionSegment(SegmentBase):
    segment_type: Literal["solution"] = "solution"
    solution_for_question_id: Optional[str] = None
    solution_steps: List[str] = Field(default_factory=list)
    referenced_formula_ids: List[str] = Field(default_factory=list)
    link_confidence: float = 1.0  # For fuzzy matches
    match_method: Optional[str] = None  # e.g. "exact_question_number", "equation_anchor", "text_similarity"
    is_synthetic: bool = False  # True if generated by LLM (not from book)
    solution_origin: Literal["book", "synthetic", "book+synthetic"] = "book"
    is_concept_check: bool = False  # True if from concept-check section
    validation_score: Optional[float] = None  # Score from multi-agent validation (0.0-1.0)


class DerivationSegment(SegmentBase):
    segment_type: Literal["derivation"] = "derivation"
    steps: List[str] = Field(default_factory=list)
    derived_from_formula_ids: List[str] = Field(default_factory=list)  # Formulas used as input
    derived_to_formula_id: Optional[str] = None  # Final formula produced
    referenced_formula_ids: List[str] = Field(default_factory=list)
    link_type: Literal["EXPLAINS", "REFERENCES"] = "EXPLAINS"


class CalculationSegment(SegmentBase):
    segment_type: Literal["calculation"] = "calculation"
    steps: List[str] = Field(default_factory=list)
    referenced_formula_ids: List[str] = Field(default_factory=list)
    output_variables: List[str] = Field(default_factory=list)  # What the calculation solves for


class HeaderSegment(SegmentBase):
    segment_type: Literal["header"] = "header"
    level: int = 1
    header_number: Optional[str] = None


class ExplanatoryTextSegment(SegmentBase):
    segment_type: Literal["explanatory_text"] = "explanatory_text"
    text_type: Optional[Literal["definition", "explanation", "description", "prose"]] = None


class ReferenceStubSegment(SegmentBase):
    segment_type: Literal["reference_stub"] = "reference_stub"
    ref_type: Literal["table", "figure", "appendix", "exhibit", "section", "chapter", "unknown"] = "unknown"
    ref_id_text: str
    target_unknown: bool = True
    source_segment_id: Optional[str] = None
    snippet: Optional[str] = None


class ChapterMetadata(BaseModel):
    chapter_number: str
    chapter_title: Optional[str] = None
    chapter_level: Optional[int] = None  # 1 = chapter, 2 = subchapter
    parent_chapter: Optional[str] = None
    solutions_present: bool = False
    solution_location: Optional[Literal[
        "in_chapter",
        "end_of_chapter",
        "appendix",
        "concept_check_only",
        "selected_answers",
        "none"
    ]] = None


class SegmentationOutput(BaseModel):
    metadata: Dict[str, Any] = Field(default_factory=dict)
    chapters: List[ChapterMetadata] = Field(default_factory=list)
    segments: List[Union[
        FormulaSegment, WorkedExampleSegment, QuestionSegment, SolutionSegment,
        DerivationSegment, CalculationSegment, HeaderSegment, ExplanatoryTextSegment, ReferenceStubSegment, SegmentBase
    ]]
    edges: List[Edge] = Field(default_factory=list)


SegmentationOutput.model_rebuild()
